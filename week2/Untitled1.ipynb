{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import ml_metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation_funcs import performance_accumulation_pixel\n",
    "from evaluation_funcs import performance_evaluation_pixel\n",
    "from bbox_iou import bbox_iou\n",
    "\n",
    "## PARAMETERS ##\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "if cfg['colorspace'] == 'HSV' :\n",
    "    COLORSPACE = cv.COLOR_BGR2HSV\n",
    "elif cfg['colorspace'] == 'YUV' :\n",
    "    COLORSPACE = cv.COLOR_BGR2YUV\n",
    "elif cfg['colorspace'] == 'LAB' :\n",
    "    COLORSPACE = cv.COLOR_BGR2Lab\n",
    "\n",
    "NBINS = cfg['nbins']    # Number of bins (from 0 to 255)\n",
    "DIVISIONS = cfg['divs'] # Number of divisions per dimension [2,4,8,...]\n",
    "DIST_METRIC= cfg['dist'] #'euclidean' 'chisq' or 'hellinger'\n",
    "BG_REMOVAL = cfg['bgrm'] # 1, 2 or 3 bg removal method\n",
    "QUERY_SET= cfg['queryset'] # Input query set\n",
    "\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS ##\n",
    "def text_removal_mask(img_gray, name, strel, strel_pd, num_cols, coords, bg_mask):\n",
    "\n",
    "    if np.shape(bg_mask)[0] == 2:\n",
    "        print(\"HOLA\")\n",
    "        plt.imshow(bg_mask[0])\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(bg_mask[1])\n",
    "        plt.show()\n",
    "\n",
    "    # Obtain image dimensions\n",
    "    height,width = img_gray.shape[:2]\n",
    "    \n",
    "    # Create variable where final mask will be stored\n",
    "    f_mask = np.ones(shape=(height,width))\n",
    "    f_mask.fill(255)\n",
    "    \n",
    "    # Boundaries of the analyzed area\n",
    "    min_a = round(width/2)\n",
    "    max_a = round(min_a + num_cols)\n",
    "    \n",
    "    # Store pixel values in the analyzed area\n",
    "    values_t = np.zeros(shape=(max_a-min_a, max_a-min_a))\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for p in range(min_a, max_a):\n",
    "        # Per each column, compute number of ocurrences of every pixel value\n",
    "        col = img_gray[:,p]\n",
    "        \n",
    "        # Pixel values and number of ocurrences for the whole column\n",
    "        values = pd.Series(col).value_counts().keys().tolist()\n",
    "        \n",
    "        # Get highest pixel values (most frequent ones)\n",
    "        values_t[0:4,i] = values[0:4]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    j = 0\n",
    "    w = 0\n",
    "    h = 0\n",
    "\n",
    "    while((w*h < 20000 or w*h > 1000000 or w<h) and j < num_cols):\n",
    "\n",
    "        level = round(np.mean(values_t[j,:]))\n",
    "        \n",
    "        if level <= 128:\n",
    "            final_img = cv.morphologyEx(img_gray, cv.MORPH_OPEN, strel)\n",
    "            mask = (final_img >= max(0,level-1)) * (final_img <= level+1)\n",
    "            mask = mask.astype(np.uint8)\n",
    "            mask *= 255\n",
    "        elif level > 128:\n",
    "            final_img = cv.morphologyEx(img_gray, cv.MORPH_CLOSE, strel)\n",
    "            mask = (final_img >= max(0,level-1)) * (final_img <= level+1)\n",
    "            mask = mask.astype(np.uint8)\n",
    "            mask *= 255\n",
    "\n",
    "        #mask = cv.morphologyEx(mask, cv.MORPH_OPEN, np.ones((2,2), np.uint8))   \n",
    "\n",
    "        if np.sum(mask) != 0:\n",
    "\n",
    "            # Find contours of created mask\n",
    "            contours,_ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "\n",
    "            # Find largest contour (it will contain the text bounding box)\n",
    "            contour_sizes = [(cv.contourArea(contour), contour) for contour in contours]\n",
    "            largest_contour = max(contour_sizes, key=lambda x: x[0])[1]\n",
    "            \n",
    "            # Find bounding box belonging to detected contour\n",
    "            (x,y,w,h) = cv.boundingRect(largest_contour)\n",
    "\n",
    "            # Draw bounding boxes coordinates on original image to visualize it\n",
    "            cv.rectangle(img_gray, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            \n",
    "            # Bboxes coordinates of the text positions\n",
    "            tlx = x\n",
    "            tly = y\n",
    "            brx = x + w\n",
    "            bry = y + h\n",
    "\n",
    "            # Extract bounding boxes pixels \n",
    "            bb_size = w*h\n",
    "\n",
    "        j+=1\n",
    "\n",
    "    # Add bboxes coordinates to a list of lists\n",
    "    coords.append([(tlx,tly,brx,bry)])\n",
    "\n",
    "    # Create new mask with bboxes coordinates\n",
    "    # Bboxes pixels are black (text), white otherwise (painting)\n",
    "    f_mask[y:y + h, x:x + w] = 0\n",
    "\n",
    "    cv.imwrite('results/'+ name + 'txtrmask.png', f_mask)\n",
    "    \n",
    "    return f_mask, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(img,name):\n",
    "    height, width = np.shape(img)\n",
    "\n",
    "    # METHOD 3 BASED ON MORPHOLOGY\n",
    "    if(BG_REMOVAL==3):\n",
    "        # Compute morphological gradient by dilation (keep inner edges to remove wall posters)\n",
    "        kernel = np.ones((40,40),np.uint8)\n",
    "        img_dilation = cv.dilate(img,kernel,iterations = 1)\n",
    "        img_gradient = img_dilation - img\n",
    "        \n",
    "        # Thresholding\n",
    "        _,img_th = cv.threshold(img_gradient, 30, 255, cv.THRESH_BINARY)\n",
    "        #retval,mask_img = cv.threshold(final_img, 30, 255, cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "\n",
    "        # Computing external contours\n",
    "        contours, _ = cv.findContours(img_th, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        img_contour = np.zeros_like(img_th)\n",
    "        cv.drawContours(img_contour, contours, -1, 255, -1)\n",
    "        \n",
    "        # Opening to remove wall posters\n",
    "        kernel = np.ones((100,200),np.uint8)\n",
    "        mask = cv.morphologyEx(img_contour, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Avoid all zeros image\n",
    "        if(mask.any()==0):\n",
    "            mask = img_contour\n",
    "\n",
    "    # METHODS 1 AND 2 BASED ON COLOR THRESHOLDING\n",
    "    else:\n",
    "        # Computes channels in chosen color space\n",
    "        c0 = img[:,:,0]\n",
    "        c1 = img[:,:,1]\n",
    "        c2 = img[:,:,2]\n",
    "            \n",
    "        # Percentage defining number of pixels per every portion of the image\n",
    "        percent_c0 = 0.02\n",
    "        percent_c1 = 0.03\n",
    "        percent_c2 = 0.02\n",
    "        \n",
    "        # Computes the amount of pixels per every channel\n",
    "        aop_h_c0 = int(round(percent_c0 * height))\n",
    "        aop_w_c0 = int(round(percent_c0 * width))\n",
    "        \n",
    "        aop_h_c1 = int(round(percent_c1 * height))\n",
    "        aop_w_c1 = int(round(percent_c1 * width))\n",
    "        \n",
    "        aop_h_c2 = int(round(percent_c2 * height))\n",
    "        aop_w_c2 = int(round(percent_c2 * width))\n",
    "        \n",
    "        # Defines image portions to get background pixels from\n",
    "        portionc0_1 = c0[0:aop_h_c0, 0:width]\n",
    "        portionc1_1 = c1[0:aop_h_c1, 0:width]\n",
    "        portionc2_1 = c2[0:aop_h_c2, 0:width]\n",
    "        \n",
    "        if(BG_REMOVAL == 1) :\n",
    "            # Method 1\n",
    "            portionc0_2 = c0[height - aop_h_c0:height, 0:width]\n",
    "            portionc1_2 = c1[height - aop_h_c1:height, 0:width]\n",
    "            portionc2_2 = c2[height - aop_h_c2:height, 0:width]\n",
    "        elif(BG_REMOVAL == 2):    \n",
    "            # Method 2\n",
    "            portionc0_2 = c0[0:height,0:aop_w_c0]\n",
    "            portionc1_2 = c1[0:height,0:aop_w_c1]\n",
    "            portionc2_2 = c2[0:height,0:aop_w_c2]\n",
    "        \n",
    "        # Computes minimum and max values per every portion and channel\n",
    "        min_c0_1 = int(np.amin(portionc0_1))\n",
    "        min_c1_1 = int(np.amin(portionc1_1))\n",
    "        min_c2_1 = int(np.amin(portionc2_1))\n",
    "        \n",
    "        min_c0_2 = int(np.amin(portionc0_2))\n",
    "        min_c1_2 = int(np.amin(portionc1_2))\n",
    "        min_c2_2 = int(np.amin(portionc2_2))\n",
    "        \n",
    "        max_c0_1 = int(np.amax(portionc0_1))\n",
    "        max_c1_1 = int(np.amax(portionc1_1))\n",
    "        max_c2_1 = int(np.amax(portionc2_1))\n",
    "        \n",
    "        max_c0_2 = int(np.amax(portionc0_2))\n",
    "        max_c1_2 = int(np.amax(portionc1_2))\n",
    "        max_c2_2 = int(np.amax(portionc2_2))\n",
    "        \n",
    "        min_c0 = min(min_c0_1, min_c0_2)\n",
    "        min_c1 = min(min_c1_1, min_c1_2)\n",
    "        min_c2 = min(min_c2_1, min_c2_2)\n",
    "        \n",
    "        max_c0 = max(max_c0_1, max_c0_2)\n",
    "        max_c1 = max(max_c1_1, max_c1_2)\n",
    "        max_c2 = max(max_c2_1, max_c2_2)\n",
    "        \n",
    "        # Computes and saves the mask by thresholding every channel in the chosen color space\n",
    "        mask = 255 - (cv.inRange(img,(min_c0, min_c1, min_c2),(max_c0, max_c1, max_c2)))\n",
    "\n",
    "    # Save mask\n",
    "    cv.imwrite('masks/' + name + '.png', mask)\n",
    "    \n",
    "    # Read ground truth\n",
    "    g_t = cv.imread('../qs/' + QUERY_SET + '/' + name + '.png', cv.IMREAD_COLOR)\n",
    "    g_t = cv.cvtColor(g_t, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    pixelTP, pixelFP, pixelFN, pixelTN = performance_accumulation_pixel(mask,g_t)\n",
    "    pixel_precision, pixel_accuracy, pixel_specificity, pixel_sensitivity = performance_evaluation_pixel(pixelTP, pixelFP, pixelFN, pixelTN)\n",
    "    F1 = 2*pixel_precision*pixel_sensitivity/(pixel_precision+pixel_sensitivity)\n",
    "    \n",
    "    eval_metrics = [pixel_precision, pixel_accuracy, pixel_specificity, pixel_sensitivity, F1]\n",
    "    '''\n",
    "    print(\"Precision: \"+str(pixel_precision))\n",
    "    print(\"Accuracy: \"+str(pixel_accuracy))\n",
    "    print(\"Specificity: \"+str(pixel_specificity))\n",
    "    print(\"Recall (sensitivity): \"+str(pixel_sensitivity))\n",
    "    print(\"F1: \"+str(F1))\n",
    "    '''\n",
    "\n",
    "    # DETECT IF THERE ARE TWO IMAGES\n",
    "    # First method: check if the central mask is black\n",
    "    central_column = round(width/2)\n",
    "    central_column_mean = np.mean(mask[:,central_column:central_column+1])\n",
    "    \n",
    "    # If central column is not zero, analyze some extra columns\n",
    "    # From 0.25 to 0.75 of the image, with a step of 100px\n",
    "    if central_column_mean != 0:\n",
    "        for i in range(round(0.5*(width/2)), round(1.5*(width/2)), 100):\n",
    "            central_column_mean = np.mean(mask[:,i:i+1])\n",
    "            if (central_column_mean == 0): # If found, exit for and keep central_column\n",
    "                central_column = i\n",
    "                break\n",
    "\n",
    "    # If after the second attempt two masks are detected \n",
    "    if central_column_mean == 0:\n",
    "        # Generate white masks\n",
    "        mask_left = np.ones((height,width),np.uint8)\n",
    "        mask_right = np.ones((height,width),np.uint8)\n",
    "        \n",
    "        # Compute\n",
    "        mask_left[:,central_column:width] = 0\n",
    "        mask_right[:,0:central_column] = 0\n",
    "        mask_left = mask_left*mask\n",
    "        mask_right = mask_right*mask\n",
    "        mask = [mask_left, mask_right]        \n",
    "    \n",
    "    return mask, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img,mask):\n",
    "\n",
    "#Extracts feature vector from image. The returned vecor consists of the 1D histograms of\n",
    "# each of the image channels concatenated.\n",
    "    \n",
    "    # Mask preprocessing\n",
    "    if mask is not None:\n",
    "        indices = np.where(mask != [0])\n",
    "        img = img[min(indices[0]):max(indices[0]),min(indices[1]):max(indices[1])]\n",
    "        mask = mask[min(indices[0]):max(indices[0]),min(indices[1]):max(indices[1])]\n",
    "\n",
    "    # Level 0 histograms:\n",
    "    hist_img = []\n",
    "    npx = img.shape[0]*img.shape[1]\n",
    "    hist_1 = cv.calcHist([img],[0],mask,[NBINS],[0,256])/npx \n",
    "    hist_2 = cv.calcHist([img],[1],mask,[NBINS],[0,256])/npx\n",
    "    hist_3 = cv.calcHist([img],[2],mask,[NBINS],[0,256])/npx\n",
    "    hists = np.concatenate((hist_1,hist_2,hist_3))\n",
    "    hist_img.append(hists)\n",
    "\n",
    "    # Multilevel histograms\n",
    "    for i in range(0,DIVISIONS):\n",
    "        for j in range(0,DIVISIONS):\n",
    "            # Compute the normalized histograms\n",
    "            subimg = img[i*round(img.shape[0]/DIVISIONS):(i+1)*round(img.shape[0]/DIVISIONS)-1, \n",
    "                         j*round(img.shape[1]/DIVISIONS):(j+1)*round(img.shape[1]/DIVISIONS)-1]\n",
    "            if mask is not None :\n",
    "                submask = mask[i*round(img.shape[0]/DIVISIONS):(i+1)*round(img.shape[0]/DIVISIONS)-1, \n",
    "                            j*round(img.shape[1]/DIVISIONS):(j+1)*round(img.shape[1]/DIVISIONS)-1]\n",
    "            else :\n",
    "                submask = None\n",
    "            npx = subimg.shape[0]*subimg.shape[1]\n",
    "            hist_1 = cv.calcHist([subimg],[0],submask,[NBINS],[0,256])/npx \n",
    "            hist_2 = cv.calcHist([subimg],[1],submask,[NBINS],[0,256])/npx\n",
    "            hist_3 = cv.calcHist([subimg],[2],submask,[NBINS],[0,256])/npx\n",
    "            hists = np.concatenate((hist_1,hist_2,hist_3))\n",
    "            hist_img.append(hists)\n",
    "    flat_list = []\n",
    "    for sublist in hist_img:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queries, database, distance, k):\n",
    "\n",
    "# For each of the queries, searches for the K  most similar images in the database. The\n",
    "#decision is based on the feature vectors and a distance or similarity measure (Euclidean\n",
    "# distance and Hellinger Kernel similarity. Returns a 2D array containing the results of\n",
    "#the search for each of the queries.\n",
    "\n",
    "    final_ranking = np.zeros((len(queries), k), dtype=float)\n",
    "    \n",
    "    if(distance == \"euclidean\"):\n",
    "        for i in range(0, len(queries)):\n",
    "            ranking = np.ones((k, 2), dtype=float) * 9999\n",
    "            for j in range(0, len(database)):\n",
    "                # Compute the distance metric\n",
    "                dist = sum(pow(abs(np.array(database[j]) - np.array(queries[i])), 2))\n",
    "                # Check the ranking and update it\n",
    "                if (dist < max(ranking[:, 1])):\n",
    "                    # Add the distance and the id to the db\n",
    "                    idx = np.argmax(ranking[:, 1])\n",
    "                    ranking[idx, 0] = j\n",
    "                    ranking[idx, 1] = dist\n",
    "            # Store the closest K images\n",
    "            for j in range(0, k):\n",
    "                idx = np.argmin(ranking[:, 1])\n",
    "                final_ranking[i, j] = ranking[idx, 0]\n",
    "                ranking[idx, :] = [9999, 9999]\n",
    "\n",
    "    if(distance == \"chisq\"):\n",
    "            for i in range(0, len(queries)):\n",
    "                ranking = np.ones((k, 2), dtype=float) * 9999\n",
    "                for j in range(0, len(database)):\n",
    "                    # Compute the distance metric\n",
    "                    dist = sum( np.divide(pow(abs(database[j] - queries[i]), 2), (database[j] + queries[i]), out=np.zeros_like(database[j]), where=queries[i]!=0) )\n",
    "                    # Check the ranking and update it\n",
    "                    if (dist < max(ranking[:, 1])):\n",
    "                        # Add the distance and the id to the db\n",
    "                        idx = np.argmax(ranking[:, 1])\n",
    "                        ranking[idx, 0] = j\n",
    "                        ranking[idx, 1] = dist\n",
    "                # Store the closest K images\n",
    "                for j in range(0, k):\n",
    "                    idx = np.argmin(ranking[:, 1])\n",
    "                    final_ranking[i, j] = ranking[idx, 0]\n",
    "                    ranking[idx, :] = [9999, 9999]\n",
    "\n",
    "    if(distance == \"hellinger\"):\n",
    "        for i in range(0, len(queries)):\n",
    "            ranking = np.zeros((k, 2), dtype=float)\n",
    "            for j in range(0, len(database)):\n",
    "                # Compute the distance metric\n",
    "                dist = np.sum(np.sqrt(np.multiply(np.array(database[j]),np.array(queries[i]))))\n",
    "                # Check the ranking and update it\n",
    "                if (dist > min(ranking[:, 1])):\n",
    "                    # Add the distance and the id to the db\n",
    "                    idx = np.argmin(ranking[:, 1])\n",
    "                    ranking[idx, 0] = j\n",
    "                    ranking[idx, 1] = dist\n",
    "            # Store the closest K images\n",
    "            for j in range(0, k):\n",
    "                idx = np.argmax(ranking[:, 1])\n",
    "                final_ranking[i, j] = ranking[idx, 0]\n",
    "                ranking[idx, :] = [0, 0]\n",
    "\n",
    "    return final_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database has 279 images\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d54c0bb8ed50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mquery_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mquery_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a0c4ed778ca8>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(img, mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "database = []\n",
    "for f in sorted(glob.glob('../database/*.jpg')):\n",
    "    img = cv.imread(f, cv.IMREAD_COLOR)\n",
    "    img = cv.cvtColor(img, COLORSPACE)\n",
    "    database.append(extract_features(img,None))\n",
    "print('Database has ' + str(len(database)) + ' images')\n",
    "\n",
    "queries = []\n",
    "\n",
    "qs_l = '../qs/' + QUERY_SET + '/*.jpg'\n",
    "\n",
    "# Evaluation metrics storing arrays\n",
    "precision = np.zeros(len(glob.glob(qs_l)))\n",
    "recall = np.zeros(len(glob.glob(qs_l)))\n",
    "fscore = np.zeros(len(glob.glob(qs_l)))\n",
    "iou = np.zeros(len(glob.glob(qs_l)))\n",
    "\n",
    "i=0\n",
    "\n",
    "# Text removal variables\n",
    "# Structuring element\n",
    "strel = np.ones((15,15), np.uint8)\n",
    "\n",
    "# Structuring element used after text removal\n",
    "strel_pd = np.ones((20,20),np.uint8)\n",
    "\n",
    "# Number of columns considered from the center of the image towards the right\n",
    "num_cols = 6\n",
    "\n",
    "# List to store detected bounding boxes coordinates\n",
    "coords = []\n",
    "\n",
    "for f in sorted(glob.glob(qs_l)):\n",
    "    name = os.path.splitext(os.path.split(f)[1])[0]\n",
    "    im = cv.imread(f, cv.IMREAD_COLOR)\n",
    "    img = cv.cvtColor(im, COLORSPACE)\n",
    "\n",
    "    # NO BACKGROUND\n",
    "    if QUERY_SET == 'qsd1_w1' or QUERY_SET == 'qst1_w1' or QUERY_SET == 'qsd1_w2' or QUERY_SET == 'qst1_w2':\n",
    "        bg_mask = None\n",
    "    # BACKGROUND REMOVAL\n",
    "    elif QUERY_SET == 'qsd2_w1' or QUERY_SET == 'qst2_w1' or QUERY_SET == 'qsd2_w2' or QUERY_SET == 'qst2_w2':\n",
    "        if BG_REMOVAL==3:\n",
    "            img_gray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\n",
    "            bg_mask, eval_metrics = compute_mask(img_gray,name)\n",
    "        else:\n",
    "            bg_mask, eval_metrics = compute_mask(img,name)\n",
    "        precision[i] = eval_metrics[0]\n",
    "        recall[i] = eval_metrics[3]\n",
    "        fscore[i] = eval_metrics[4]\n",
    "\n",
    "    # TEXT REMOVAL\n",
    "    if QUERY_SET == 'qsd1_w2' or QUERY_SET == 'qst1_w2' or QUERY_SET == 'qsd2_w2' or QUERY_SET == 'qst2_w2':\n",
    "        img_gray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\n",
    "        # Use the mask created (image without background) to indicate search area\n",
    "        mask, pred_coords = text_removal_mask(img_gray, name, strel, strel_pd, num_cols, coords, bg_mask)\n",
    "        mask = mask.astype(np.uint8)\n",
    "    else:\n",
    "        mask = bg_mask\n",
    "\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "    # Iterate the masks (1 or 2 according to the images)\n",
    "    query_data = []\n",
    "    for m in mask:\n",
    "        query_data.append(extract_features(img,m))\n",
    "\n",
    "    queries.append(query_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178, 1159)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-df6aa0f83911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2699\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2700\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    636\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    637\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.shape(bg_mask))\n",
    "plt.imshow(bg_mask[0])\n",
    "plt.show()\n",
    "plt.imshow(bg_mask[1])\n",
    "plt.show()\n",
    "\n",
    "# Iterate the masks (1 or 2 according to the images)\n",
    "query_data = []\n",
    "for m in bg_mask:\n",
    "    query_data.append(extract_features(img,m))\n",
    "\n",
    "queries.append(query_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 19392, 1)\n",
      "(2,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(query_data))\n",
    "print(np.shape(queries))\n",
    "print(str(len(queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_SET == 'qsd2_w1':\n",
    "    print('Query set has ' + str(len(queries)) + ' images')\n",
    "    print('Precision: ' + str(np.mean(precision)))\n",
    "    print('Recall: ' + str(np.mean(recall)))\n",
    "    print('F-measure: ' + str(np.mean(fscore)))\n",
    "\n",
    "if (QUERY_SET == 'qsd1_w2' or QUERY_SET == 'qst1_w2'):\n",
    "    realcoords = pickle.load(open(QUERY_SET + '/text_boxes.pkl','rb'))\n",
    "    i = 0\n",
    "    for i in range(0, len(realcoords)):\n",
    "        real = coords[i][0]\n",
    "        predicted = realcoords[i][0]\n",
    "        iou[i] = bbox_iou(real, predicted)\n",
    "        i += 1\n",
    "    print('Mean IOU: ' + str(np.mean(iou)))\n",
    "\n",
    "    ## WRITE PREDICTED BOUNDING BOXES ##\n",
    "    pickle.dump(pred_coords, open('../qs/' + QUERY_SET + '/pred_bboxes.pkl','wb'))\n",
    "\n",
    "## ADD QSD2_W2 AND QST2_W2\n",
    "\n",
    "## SEARCH FOR THE QUERIES IN THE DB ##\n",
    "final_ranking = search(queries, database, DIST_METRIC, K)\n",
    "print('FINAL RANKING:')\n",
    "print(final_ranking)\n",
    "\n",
    "## EVALUATION USING MAP@K ##\n",
    "if QUERY_SET == 'qsd1_w1' or QUERY_SET == 'qsd2_w1'  or QUERY_SET == 'qsd1_w2' or QUERY_SET == 'qsd2_w2':\n",
    "    gt = pickle.load(open('../qs/' + QUERY_SET + '/gt_corresps.pkl','rb'))\n",
    "    mapk_ = ml_metrics.mapk(gt,final_ranking.tolist(),K)\n",
    "    print('MAP@K = '+ str(mapk_))\n",
    "\n",
    "## WRITE OUTPUT FILES ##\n",
    "pickle.dump(final_ranking.tolist(), open('../qs/' + QUERY_SET + '/actual_corresps.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
